{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Makarand\n",
      "[nltk_data]     Thorat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "import operator\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml(file_path):\n",
    "    # Add a dummy root element to the XML content\n",
    "    with open(file_path, \"r\") as f:\n",
    "        xml_content = f.read()\n",
    "        xml_content = \"<root>\" + xml_content + \"</root>\"\n",
    "    \n",
    "    # Parse the XML content\n",
    "    root = ET.fromstring(xml_content)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml_query(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract document information\n",
    "def extract_documents(xml_root):\n",
    "    documents = []\n",
    "    for doc in xml_root.findall(\"doc\"):\n",
    "        doc_info = {}\n",
    "        docno_element = doc.find(\"docno\")\n",
    "        title_element = doc.find(\"title\")\n",
    "        author_element = doc.find(\"author\")\n",
    "        bib_element = doc.find(\"bib\")\n",
    "        text_element = doc.find(\"text\")\n",
    "        \n",
    "        # Check if elements exist and have text before accessing their text\n",
    "        if docno_element is not None and docno_element.text is not None:\n",
    "            doc_info[\"docno\"] = docno_element.text.strip()\n",
    "        else:\n",
    "            doc_info[\"docno\"] = \"\"\n",
    "        \n",
    "        if title_element is not None and title_element.text is not None:\n",
    "            doc_info[\"title\"] = title_element.text.strip()\n",
    "        else:\n",
    "            doc_info[\"title\"] = \"\"\n",
    "        \n",
    "        if author_element is not None and author_element.text is not None:\n",
    "            doc_info[\"author\"] = author_element.text.strip()\n",
    "        else:\n",
    "            doc_info[\"author\"] = \"\"\n",
    "        \n",
    "        if bib_element is not None and bib_element.text is not None:\n",
    "            doc_info[\"bib\"] = bib_element.text.strip()\n",
    "        else:\n",
    "            doc_info[\"bib\"] = \"\"\n",
    "        \n",
    "        if text_element is not None and text_element.text is not None:\n",
    "            doc_info[\"text\"] = text_element.text.strip()\n",
    "        else:\n",
    "            doc_info[\"text\"] = \"\"\n",
    "        \n",
    "        documents.append(doc_info)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract query information\n",
    "def extract_query(xml_root):\n",
    "    query = []\n",
    "    for doc in xml_root.findall(\"top\"):\n",
    "        query_info = {}\n",
    "        no_element = doc.find(\"num\")\n",
    "        title_element = doc.find(\"title\")\n",
    "        \n",
    "        # Check if elements exist and have text before accessing their text\n",
    "        if no_element is not None and no_element.text is not None:\n",
    "            query_info[\"num\"] = no_element.text.strip()\n",
    "        else:\n",
    "            query_info[\"num\"] = \"\"\n",
    "        \n",
    "        if title_element is not None and title_element.text is not None:\n",
    "            query_info[\"title\"] = title_element.text.strip()\n",
    "        else:\n",
    "            query_info[\"title\"] = \"\"\n",
    "        \n",
    "        \n",
    "        query.append(query_info)\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_root = parse_xml(\"cran.all.1400.xml\")\n",
    "query_root = parse_xml_query(\"cran.qry.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = extract_documents(documents_root)\n",
    "queries = extract_query(query_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PreProcessing \n",
    "\n",
    "\n",
    "def remove_punctuation(sentence):\n",
    "    return re.sub(r'[^\\w\\s]', '', sentence)\n",
    "\n",
    "def get_tokenized_list(doc_text):\n",
    "    tokens = nltk.word_tokenize(doc_text)\n",
    "    return tokens\n",
    "# # Stemming\n",
    "# def word_stemmer(token_list):\n",
    "#     ps = nltk.stem.PorterStemmer()\n",
    "#     stemmed = []\n",
    "#     for words in token_list:\n",
    "#         stemmed.append(ps.stem(words))\n",
    "#     return stemmed\n",
    "\n",
    "#removing stop words\n",
    "def remove_stopwords(doc_text):\n",
    "    cleaned_text = []\n",
    "    for words in doc_text:\n",
    "        if words not in stop_words:\n",
    "            cleaned_text.append(words)\n",
    "    return cleaned_text\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return lemmatized_words\n",
    "\n",
    "def stem_lem(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        lemmatized_token = lemmatizer.lemmatize(token)\n",
    "        stemmed_token = stemmer.stem(lemmatized_token)\n",
    "        processed_tokens.append(stemmed_token)\n",
    "    return processed_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing our doc\n",
    "\n",
    "\n",
    "def preprocess(element,flag):\n",
    "\n",
    "    cleaned_corpus = [ ]\n",
    "\n",
    "    element = remove_punctuation(element)\n",
    "    #print(\"Punctuation removal :\",element)\n",
    "    type(element)\n",
    "    tokens = get_tokenized_list(element)\n",
    "    #print(\"token creation\",tokens)\n",
    "    doc_text = remove_stopwords(tokens)\n",
    "    #print(\"stop word removal\",doc_text)\n",
    "    for w in stem_lem(doc_text):\n",
    "        cleaned_corpus.append(w)\n",
    "    #cleaned_corpus = ''.join(cleaned_corpus)\n",
    "    #print(\"cleaned_corpus\",cleaned_corpus) \n",
    "    if(flag==1):\n",
    "        answer=''\n",
    "        for x in cleaned_corpus:\n",
    "            answer += \" \"  + x\n",
    "        return answer\n",
    "    else:\n",
    "        return cleaned_corpus\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = [doc['docno'] for doc in documents]\n",
    "query_ids = [query['num'] for query in queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [preprocess(doc['text'],0) for doc in documents]  \n",
    "queries = [preprocess(query['title'],0) for query in queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Okapi(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output_bm25sl.txt', 'w') as f:\n",
    "    # For each query\n",
    "    for i, query in enumerate(queries):\n",
    "        # Get the scores\n",
    "        doc_scores = bm25.get_scores(query)\n",
    "\n",
    "        # Pair doc_ids with their scores\n",
    "        scored_docs = zip(doc_ids, doc_scores)\n",
    "\n",
    "        # Sort the docs by score in descending order\n",
    "        sorted_docs = sorted(scored_docs, key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "        # Get the top 100 docs\n",
    "        top_docs = sorted_docs[:100]\n",
    "\n",
    "        # Write the top docs to the file\n",
    "        for rank, (doc_id, score) in enumerate(top_docs):\n",
    "            f.write(f\"{query_ids[i]} Q0 {doc_id} {rank+1} {round(score,2)} 1\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
