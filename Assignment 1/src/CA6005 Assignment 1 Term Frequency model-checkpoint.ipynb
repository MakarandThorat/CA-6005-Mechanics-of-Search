{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml(file_path):\n",
    "    # Add a dummy root element to the XML content\n",
    "    with open(file_path, \"r\") as f:\n",
    "        xml_content = f.read()\n",
    "        xml_content = \"<root>\" + xml_content + \"</root>\"\n",
    "    \n",
    "    # Parse the XML content\n",
    "    root = ET.fromstring(xml_content)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml_query(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract document information\n",
    "def extract_documents(xml_root):\n",
    "    documents = []\n",
    "    for doc in xml_root.findall(\"doc\"):\n",
    "        doc_info = {}\n",
    "        docno_element = doc.find(\"docno\")\n",
    "        title_element = doc.find(\"title\")\n",
    "        author_element = doc.find(\"author\")\n",
    "        bib_element = doc.find(\"bib\")\n",
    "        text_element = doc.find(\"text\")\n",
    "        \n",
    "        # Check if elements exist and have text before accessing their text\n",
    "        if docno_element is not None and docno_element.text is not None:\n",
    "            doc_info[\"docno\"] = docno_element.text.strip()\n",
    "        else:\n",
    "            doc_info[\"docno\"] = \"\"\n",
    "        \n",
    "        if title_element is not None and title_element.text is not None:\n",
    "            doc_info[\"title\"] = title_element.text.strip()\n",
    "        else:\n",
    "            doc_info[\"title\"] = \"\"\n",
    "        \n",
    "        if author_element is not None and author_element.text is not None:\n",
    "            doc_info[\"author\"] = author_element.text.strip()\n",
    "        else:\n",
    "            doc_info[\"author\"] = \"\"\n",
    "        \n",
    "        if bib_element is not None and bib_element.text is not None:\n",
    "            doc_info[\"bib\"] = bib_element.text.strip()\n",
    "        else:\n",
    "            doc_info[\"bib\"] = \"\"\n",
    "        \n",
    "        if text_element is not None and text_element.text is not None:\n",
    "            doc_info[\"text\"] = text_element.text.strip()\n",
    "        else:\n",
    "            doc_info[\"text\"] = \"\"\n",
    "        \n",
    "        documents.append(doc_info)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract query information\n",
    "def extract_query(xml_root):\n",
    "    query = []\n",
    "    for doc in xml_root.findall(\"top\"):\n",
    "        query_info = {}\n",
    "        no_element = doc.find(\"num\")\n",
    "        title_element = doc.find(\"title\")\n",
    "        \n",
    "        # Check if elements exist and have text before accessing their text\n",
    "        if no_element is not None and no_element.text is not None:\n",
    "            query_info[\"num\"] = no_element.text.strip()\n",
    "        else:\n",
    "            query_info[\"num\"] = \"\"\n",
    "        \n",
    "        if title_element is not None and title_element.text is not None:\n",
    "            query_info[\"title\"] = title_element.text.strip()\n",
    "        else:\n",
    "            query_info[\"title\"] = \"\"\n",
    "        \n",
    "        \n",
    "        query.append(query_info)\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_root = parse_xml(\"cran.all.1400.xml\")\n",
    "query_root = parse_xml_query(\"cran.qry.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = extract_documents(documents_root)\n",
    "query = extract_query(query_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PreProcessing \n",
    "\n",
    "\n",
    "def remove_punctuation(sentence):\n",
    "    return re.sub(r'[^\\w\\s]', '', sentence)\n",
    "\n",
    "def get_tokenized_list(doc_text):\n",
    "    tokens = nltk.word_tokenize(doc_text)\n",
    "    return tokens\n",
    "# Stemming\n",
    "def word_stemmer(token_list):\n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    stemmed = []\n",
    "    for words in token_list:\n",
    "        stemmed.append(ps.stem(words))\n",
    "    return stemmed\n",
    "\n",
    "#removing stop words\n",
    "def remove_stopwords(doc_text):\n",
    "    cleaned_text = []\n",
    "    for words in doc_text:\n",
    "        if words not in stop_words:\n",
    "            cleaned_text.append(words)\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing our doc\n",
    "\n",
    "\n",
    "def preprocess(element,flag):\n",
    "\n",
    "    cleaned_corpus = [ ]\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "    element = remove_punctuation(element)\n",
    "    #print(\"Punctuation removal :\",element)\n",
    "    type(element)\n",
    "    tokens = get_tokenized_list(element)\n",
    "    #print(\"token creation\",tokens)\n",
    "    doc_text = remove_stopwords(tokens)\n",
    "    #print(\"stop word removal\",doc_text)\n",
    "    for w in word_stemmer(doc_text):\n",
    "        cleaned_corpus.append(w)\n",
    "    #cleaned_corpus = ''.join(cleaned_corpus)\n",
    "    #print(\"cleaned_corpus\",cleaned_corpus) \n",
    "    if(flag==1):\n",
    "        answer=''\n",
    "        for x in cleaned_corpus:\n",
    "            answer += \" \"  + x\n",
    "        return answer\n",
    "    else:\n",
    "        return cleaned_corpus\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a single list of all the document in form of a list \n",
    "cleaned_documents = []\n",
    "for i in range(0,len(documents)):\n",
    "    cleaned_documents.append(preprocess(documents[i]['text'],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final\n",
    "vectorizer = CountVectorizer()\n",
    "document_vectors = vectorizer.fit_transform(cleaned_documents)\n",
    "\n",
    "# Iterate over each query\n",
    "for query_info in query:\n",
    "    cleaned_query=[]\n",
    "    query_id = query_info['num']  # Get the query ID\n",
    "    query_text = query_info['title']  # Get the query text\n",
    "    cleaned_query.append(preprocess(query_text,1))\n",
    "    query_vector = vectorizer.transform(cleaned_query)\n",
    "    similarities = cosine_similarity(query_vector, document_vectors)[0] \n",
    "    \n",
    "    # Combine similarities with document indices\n",
    "    document_scores = list(enumerate(similarities))\n",
    "    \n",
    "    # Sort document scores based on similarity score in descending order\n",
    "    document_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Write top 100 documents for the current query to the output file\n",
    "    for rank, (doc_index, similarity_score) in enumerate(document_scores[:100], start=1):\n",
    "        doc_id = doc_index + 1  # Assuming document IDs start from 1\n",
    "        output_line = f\"{query_id} Q0 {doc_id} {rank} {round(similarity_score,2)} 1\\n\"\n",
    "        with open(\"output_TF.txt\", \"a\") as f:\n",
    "            f.write(output_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
