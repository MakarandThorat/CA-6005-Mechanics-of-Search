{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to parse the document XML file and find the root\n",
    "def parse_xml(file_path):\n",
    "    # Add a dummy root element to the XML content\n",
    "    with open(file_path, \"r\") as f:\n",
    "        xml_content = f.read()\n",
    "        xml_content = \"<root>\" + xml_content + \"</root>\"\n",
    "    \n",
    "    # Parse the XML content\n",
    "    root = ET.fromstring(xml_content)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to parse the query document as it already has root no need to add dummy root\n",
    "def parse_xml_query(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract document information\n",
    "def extract_documents(xml_root):\n",
    "    documents = []\n",
    "    for doc in xml_root.findall(\"doc\"):\n",
    "        doc_info = {}\n",
    "        docno_element = doc.find(\"docno\")\n",
    "        title_element = doc.find(\"title\")\n",
    "        author_element = doc.find(\"author\")\n",
    "        bib_element = doc.find(\"bib\")\n",
    "        text_element = doc.find(\"text\")\n",
    "        \n",
    "        # Check if elements exist and have text before accessing their text\n",
    "        if docno_element is not None and docno_element.text is not None:\n",
    "            doc_info[\"docno\"] = docno_element.text.strip()\n",
    "        else:\n",
    "            doc_info[\"docno\"] = \"\"\n",
    "        \n",
    "        if title_element is not None and title_element.text is not None:\n",
    "            doc_info[\"title\"] = title_element.text.strip()\n",
    "        else:\n",
    "            doc_info[\"title\"] = \"\"\n",
    "        \n",
    "        if author_element is not None and author_element.text is not None:\n",
    "            doc_info[\"author\"] = author_element.text.strip()\n",
    "        else:\n",
    "            doc_info[\"author\"] = \"\"\n",
    "        \n",
    "        if bib_element is not None and bib_element.text is not None:\n",
    "            doc_info[\"bib\"] = bib_element.text.strip()\n",
    "        else:\n",
    "            doc_info[\"bib\"] = \"\"\n",
    "        \n",
    "        if text_element is not None and text_element.text is not None:\n",
    "            doc_info[\"text\"] = text_element.text.strip()\n",
    "        else:\n",
    "            doc_info[\"text\"] = \"\"\n",
    "        \n",
    "        documents.append(doc_info)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract query information\n",
    "def extract_query(xml_root):\n",
    "    query = []\n",
    "    for doc in xml_root.findall(\"top\"):\n",
    "        query_info = {}\n",
    "        no_element = doc.find(\"num\")\n",
    "        title_element = doc.find(\"title\")\n",
    "        \n",
    "        # Check if elements exist and have text before accessing their text\n",
    "        if no_element is not None and no_element.text is not None:\n",
    "            query_info[\"num\"] = no_element.text.strip()\n",
    "        else:\n",
    "            query_info[\"num\"] = \"\"\n",
    "        \n",
    "        if title_element is not None and title_element.text is not None:\n",
    "            query_info[\"title\"] = title_element.text.strip()\n",
    "        else:\n",
    "            query_info[\"title\"] = \"\"\n",
    "        \n",
    "        \n",
    "        query.append(query_info)\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_root = parse_xml(\"cran.all.1400.xml\")\n",
    "query_root = parse_xml_query(\"cran.qry.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = extract_documents(documents_root)\n",
    "queries = extract_query(query_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Makarand\n",
      "[nltk_data]     Thorat\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "#PreProcessing \n",
    "\n",
    "\n",
    "def remove_punctuation(sentence):\n",
    "    return re.sub(r'[^\\w\\s]', '', sentence)\n",
    "\n",
    "def get_tokenized_list(doc_text):\n",
    "    tokens = nltk.word_tokenize(doc_text)\n",
    "    return tokens\n",
    "# Stemming\n",
    "def word_stemmer(token_list):\n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    stemmed = []\n",
    "    for words in token_list:\n",
    "        stemmed.append(ps.stem(words))\n",
    "    return stemmed\n",
    "\n",
    "#removing stop words\n",
    "def remove_stopwords(doc_text):\n",
    "    cleaned_text = []\n",
    "    for words in doc_text:\n",
    "        if words not in stop_words:\n",
    "            cleaned_text.append(words)\n",
    "    return cleaned_text\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "def lemmatize_words(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return lemmatized_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing our doc\n",
    "\n",
    "\n",
    "def preprocess(element,flag):\n",
    "\n",
    "    cleaned_corpus = [ ]\n",
    "\n",
    "    element = remove_punctuation(element)\n",
    "    #print(\"Punctuation removal :\",element)\n",
    "    type(element)\n",
    "    tokens = get_tokenized_list(element)\n",
    "    #print(\"token creation\",tokens)\n",
    "    doc_text = remove_stopwords(tokens)\n",
    "    #print(\"stop word removal\",doc_text)\n",
    "    for w in word_stemmer(doc_text):\n",
    "        cleaned_corpus.append(w)\n",
    "    #cleaned_corpus = ''.join(cleaned_corpus)\n",
    "    #print(\"cleaned_corpus\",cleaned_corpus) \n",
    "    if(flag==1):\n",
    "        answer=''\n",
    "        for x in cleaned_corpus:\n",
    "            answer += \" \"  + x\n",
    "        return answer\n",
    "    else:\n",
    "        return cleaned_corpus\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['experimental', 'investigation', 'of', 'the', 'aerodynamics', 'of', 'a', 'wing', 'in', 'a', 'slipstream', 'an', 'experimental', 'study', 'of', 'a', 'wing', 'in', 'a', 'propeller', 'slipstream', 'was', 'made', 'in', 'order', 'to', 'determine', 'the', 'spanwise', 'distribution', 'of', 'the', 'lift', 'increase', 'due', 'to', 'slipstream', 'at', 'different', 'angles', 'of', 'attack', 'of', 'the', 'wing', 'and', 'at', 'different', 'free', 'stream', 'to', 'slipstream', 'velocity', 'ratios', 'the', 'results', 'were', 'intended', 'in', 'part', 'as', 'an', 'evaluation', 'basis', 'for', 'different', 'theoretical', 'treatments', 'of', 'this', 'problem', 'the', 'comparative', 'span', 'loading', 'curves', 'together', 'with', 'supporting', 'evidence', 'showed', 'that', 'a', 'substantial', 'part', 'of', 'the', 'lift', 'increment', 'produced', 'by', 'the', 'slipstream', 'was', 'due', 'to', 'a', 'destalling', 'or', 'boundarylayercontrol', 'effect', 'the', 'integrated', 'remaining', 'lift', 'increment', 'after', 'subtracting', 'this', 'destalling', 'lift', 'was', 'found', 'to', 'agree', 'well', 'with', 'a', 'potential', 'flow', 'theory', 'an', 'empirical', 'evaluation', 'of', 'the', 'destalling', 'effects', 'was', 'made', 'for', 'the', 'specific', 'configuration', 'of', 'the', 'experiment']\n",
      "['experiment', 'investig', 'of', 'the', 'aerodynam', 'of', 'a', 'wing', 'in', 'a', 'slipstream', 'an', 'experiment', 'studi', 'of', 'a', 'wing', 'in', 'a', 'propel', 'slipstream', 'wa', 'made', 'in', 'order', 'to', 'determin', 'the', 'spanwis', 'distribut', 'of', 'the', 'lift', 'increas', 'due', 'to', 'slipstream', 'at', 'differ', 'angl', 'of', 'attack', 'of', 'the', 'wing', 'and', 'at', 'differ', 'free', 'stream', 'to', 'slipstream', 'veloc', 'ratio', 'the', 'result', 'were', 'intend', 'in', 'part', 'as', 'an', 'evalu', 'basi', 'for', 'differ', 'theoret', 'treatment', 'of', 'thi', 'problem', 'the', 'compar', 'span', 'load', 'curv', 'togeth', 'with', 'support', 'evid', 'show', 'that', 'a', 'substanti', 'part', 'of', 'the', 'lift', 'increment', 'produc', 'by', 'the', 'slipstream', 'wa', 'due', 'to', 'a', 'destal', 'or', 'boundarylayercontrol', 'effect', 'the', 'integr', 'remain', 'lift', 'increment', 'after', 'subtract', 'thi', 'destal', 'lift', 'wa', 'found', 'to', 'agre', 'well', 'with', 'a', 'potenti', 'flow', 'theori', 'an', 'empir', 'evalu', 'of', 'the', 'destal', 'effect', 'wa', 'made', 'for', 'the', 'specif', 'configur', 'of', 'the', 'experi']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=documents[0]['text']\n",
    "\n",
    "x=remove_punctuation(x)\n",
    "x=get_tokenized_list(x)\n",
    "print(x)\n",
    "x=word_stemmer(x)\n",
    "\n",
    "print(x)\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = [doc['docno'] for doc in documents]\n",
    "query_ids = [query['num'] for query in queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [preprocess(doc['text'],1) for doc in documents]  \n",
    "queries = [preprocess(query['title'],1) for query in queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "doc_vectors = vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the output file\n",
    "with open('output_vs.txt', 'w') as f:\n",
    "    # For each query\n",
    "    for i, query in enumerate(queries):\n",
    "        # Transform the query to the same vector space as the docs\n",
    "        query_vector = vectorizer.transform([query])\n",
    "\n",
    "        # Compute the cosine similarity between the query vector and the doc vectors\n",
    "        cosine_similarities = linear_kernel(query_vector, doc_vectors).flatten()\n",
    "\n",
    "        # Pair doc_ids with their scores\n",
    "        scored_docs = zip(doc_ids, cosine_similarities)\n",
    "\n",
    "        # Sort the docs by score in descending order\n",
    "        sorted_docs = sorted(scored_docs, key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "        # Get the top 100 docs\n",
    "        top_docs = sorted_docs[:100]\n",
    "\n",
    "        # Write the top docs to the file\n",
    "        for rank, (doc_id, score) in enumerate(top_docs):\n",
    "            f.write(f\"{query_ids[i]} Q0 {doc_id} {rank+1} {round(score,2)} 1\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
